# -*- coding: utf-8 -*-
"""AREAS AI - GMaps Reviews Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IcGT0kC2-i1NSaq7J_VuzjAVcPVqRxWA

# Imports
"""

# !pip install googlemaps
# !pip install googlemaps tenacity openpyxl -q
# !pip install anthropic
# !pip install --upgrade anthropic

# Imports
import googlemaps
import pandas as pd
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from google.colab import userdata, drive, files
from tenacity import retry, stop_after_attempt, wait_exponential
import math
import os
import anthropic
import glob

# Monter Google Drive
drive.mount('/content/drive')

# R√©cup√©rer la cl√© API depuis les secrets
GOOGLE_MAPS_API_KEY = userdata.get('GMAPS')
ANTHROPIC_API_KEY = userdata.get('ANTH')

print("‚úÖ Configuration OK")

"""# Fonctions Utilitaires"""

def calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculer distance en km (formule Haversine)"""
    R = 6371.0  # Rayon de la Terre en km

    lat1_rad = math.radians(lat1)
    lon1_rad = math.radians(lon1)
    lat2_rad = math.radians(lat2)
    lon2_rad = math.radians(lon2)

    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad

    a = (math.sin(dlat / 2) ** 2 +
         math.cos(lat1_rad) * math.cos(lat2_rad) *
         math.sin(dlon / 2) ** 2)

    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    return round(R * c, 2)

print("‚úÖ Utilitaires OK")

def normalize_gare_name(gare_code: str) -> str:
    """
    Convertir les codes de gare en noms complets pour le matching
    Ex: PARIS-GSL ‚Üí Gare Saint-Lazare
    """
    normalize_map = {
        'PARIS-GDE': "Gare de l'Est",
        'PARIS-GDN': 'Gare du Nord',
        'PARIS-GDL': 'Gare de Lyon',
        'PARIS-GSL': 'Gare Saint-Lazare',
        'PARIS-GMP': 'Gare Montparnasse'
    }

    # Essayer de remplacer les codes
    for code, full_name in normalize_map.items():
        if code in gare_code:
            return full_name

    # Si pas de match, retourner tel quel
    return gare_code

print("‚úÖ Normalisation des gares OK")

"""# Extraction des Reviews

## Connection Google Maps
"""

class GoogleMapsReviewsService:
    def __init__(self, api_key: str, cache_dir: str = "/content/cache"):
        self.gmaps = googlemaps.Client(key=api_key)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        print(f"‚úÖ Service initialis√©")

    def _get_cache_path(self, cache_key: str):
        safe_key = cache_key.replace('/', '_').replace(' ', '_')
        return self.cache_dir / f"{safe_key}.json"

    def _load_from_cache(self, cache_key: str):
        cache_file = self._get_cache_path(cache_key)
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None

    def _save_to_cache(self, cache_key: str, data):
        cache_file = self._get_cache_path(cache_key)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def find_nearest_to_station(self, station_name: str, keyword: str, exclude_place_ids=None):
        """
        Fallback: chercher le lieu le plus proche d'une gare par recherche g√©ographique
        AM√âLIORATION: Essaie plusieurs radius (500m ‚Üí 1000m ‚Üí 2000m)
        """
        if exclude_place_ids is None:
            exclude_place_ids = set()

        print(f"   üîÑ Fallback: recherche g√©ographique autour de {station_name}")

        try:
            # 1. G√©ocoder la gare
            geocode_result = self.gmaps.geocode(f"Gare de {station_name}", language='fr', region='fr')
            if not geocode_result:
                print(f"   ‚ùå Impossible de g√©ocoder la gare")
                return None

            location = geocode_result[0]['geometry']['location']
            lat, lng = location['lat'], location['lng']
            print(f"   üìç Gare: ({lat:.4f}, {lng:.4f})")

            # 2. Essayer plusieurs radius progressivement
            radius_list = [500, 1000, 1500, 2000, 3000, 5000, 10000]

            for radius in radius_list:
                print(f"   üîç Recherche dans un rayon de {radius}m...")

                results = self.gmaps.places_nearby(
                    location=(lat, lng),
                    radius=radius,
                    keyword=keyword,
                    language='fr'
                )

                if not results or 'results' not in results or not results['results']:
                    print(f"   ‚ö†Ô∏è Aucun r√©sultat dans {radius}m")
                    continue  # Essayer le radius suivant

                # 3. Filtrer les gares elles-m√™mes
                non_station_results = [
                    p for p in results['results']
                    if 'train_station' not in p.get('types', []) and
                      'transit_station' not in p.get('types', []) and
                      'subway_station' not in p.get('types', [])
                ]

                # 4. Filtrer les exclus
                available = [p for p in non_station_results if p['place_id'] not in exclude_place_ids]

                if not available:
                    print(f"   ‚ö†Ô∏è Tous les r√©sultats dans {radius}m d√©j√† utilis√©s")
                    continue  # Essayer le radius suivant

                # 5. Succ√®s ! Le premier est le plus proche
                closest = available[0]
                distance_km = radius / 1000
                print(f"   ‚úÖ Trouv√© √† {distance_km}km: {closest.get('name')}")
                return closest['place_id']

            # Si aucun radius n'a fonctionn√©
            print(f"   ‚ùå Aucun r√©sultat trouv√© m√™me avec 2km de rayon")
            return None

        except Exception as e:
            print(f"   ‚ùå Erreur fallback: {str(e)}")
            return None

    def _get_place_data(self, place_id: str):
        """
        R√©cup√©rer les donn√©es compl√®tes d'un lieu par son place_id
        """
        try:
            place_details = self.gmaps.place(
                place_id=place_id,
                fields=['name', 'place_id', 'formatted_address', 'geometry/location',
                        'rating', 'user_ratings_total', 'formatted_phone_number',
                        'website', 'url', 'reviews', 'business_status', 'type'],
                language='fr'
            )

            if not place_details or 'result' not in place_details:
                return None

            result = place_details['result']
            location = result['geometry']['location']

            return {
                'name': result.get('name'),
                'place_id': place_id,
                'formatted_address': result.get('formatted_address'),
                'lat': location['lat'],
                'lng': location['lng'],
                'rating': result.get('rating'),
                'user_ratings_total': result.get('user_ratings_total'),
                'phone': result.get('formatted_phone_number'),
                'website': result.get('website'),
                'url': result.get('url'),
                'reviews': result.get('reviews', []),
                'types': result.get('type', []),
                'is_duplicate': False
            }

        except Exception as e:
            print(f"   ‚ùå Erreur r√©cup√©ration donn√©es: {str(e)}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def find_restaurant(self, restaurant_name: str, city: str, exclude_place_ids=None):
        """
        Trouver un restaurant en excluant les place_ids d√©j√† utilis√©s
        ‚úÖ Filtre les gares des r√©sultats
        """
        if exclude_place_ids is None:
            exclude_place_ids = set()

        # Nettoyer le PDV
        pdv_clean = restaurant_name[3:-9].strip() if len(restaurant_name) > 12 else restaurant_name
        search_query = f"{pdv_clean} gare {city}"

        # Extraire l'enseigne pour le fallback
        enseigne = pdv_clean.split()[-1] if pdv_clean else ""

        print(f"üîç {search_query}")

        try:
            # TENTATIVE 1: Recherche textuelle
            results = self.gmaps.places(query=search_query, language='fr', region='fr')

            if not results or 'results' not in results or not results['results']:
                print(f"   ‚ö†Ô∏è Aucun r√©sultat par recherche textuelle")

                # FALLBACK: Recherche g√©ographique
                fallback_place_id = self.find_nearest_to_station(city, enseigne, exclude_place_ids)

                if not fallback_place_id:
                    print(f"‚ùå Non trouv√©")
                    return None

                restaurant_data = self._get_place_data(fallback_place_id)
                if restaurant_data:
                    restaurant_data['found_by_fallback'] = True
                    print(f"‚úÖ Trouv√© par fallback: {restaurant_data.get('name')} ({restaurant_data.get('user_ratings_total', 0)} avis)")
                return restaurant_data

            # ‚úÖ FILTRER LES GARES DES R√âSULTATS
            non_station_results = [
                place for place in results['results']
                if 'train_station' not in place.get('types', []) and
                   'transit_station' not in place.get('types', []) and
                   'subway_station' not in place.get('types', [])
            ]

            if not non_station_results:
                print(f"   ‚ö†Ô∏è Tous les r√©sultats sont des gares")

                # FALLBACK: Recherche g√©ographique
                fallback_place_id = self.find_nearest_to_station(city, enseigne, exclude_place_ids)

                if not fallback_place_id:
                    print(f"‚ùå Non trouv√©")
                    return None

                restaurant_data = self._get_place_data(fallback_place_id)
                if restaurant_data:
                    restaurant_data['found_by_fallback'] = True
                    print(f"‚úÖ Trouv√© par fallback: {restaurant_data.get('name')} ({restaurant_data.get('user_ratings_total', 0)} avis)")
                return restaurant_data

            # Filtrer les place_ids d√©j√† utilis√©s
            available_results = [
                place for place in non_station_results
                if place['place_id'] not in exclude_place_ids
            ]

            if not available_results:
                print(f"   ‚ö†Ô∏è Tous les r√©sultats non-gare d√©j√† utilis√©s")

                # FALLBACK: Recherche g√©ographique
                fallback_place_id = self.find_nearest_to_station(city, enseigne, exclude_place_ids)

                if not fallback_place_id:
                    # Dernier recours: doublon
                    print(f"   ‚ö†Ô∏è Utilisation du premier r√©sultat (doublon)")
                    place = non_station_results[0]
                    place['is_duplicate'] = True
                else:
                    restaurant_data = self._get_place_data(fallback_place_id)
                    if restaurant_data:
                        restaurant_data['found_by_fallback'] = True
                        print(f"‚úÖ Trouv√© par fallback: {restaurant_data.get('name')} ({restaurant_data.get('user_ratings_total', 0)} avis)")
                    return restaurant_data
            else:
                place = available_results[0]
                place['is_duplicate'] = False

            place_id = place['place_id']

            # R√©cup√©rer les donn√©es compl√®tes
            restaurant_data = self._get_place_data(place_id)

            if restaurant_data:
                restaurant_data['is_duplicate'] = place.get('is_duplicate', False)
                restaurant_data['found_by_fallback'] = False

                duplicate_flag = "‚ö†Ô∏è DOUBLON" if restaurant_data['is_duplicate'] else ""
                print(f"‚úÖ Trouv√©: {restaurant_data.get('name')} ({restaurant_data.get('user_ratings_total', 0)} avis) {duplicate_flag}")
                return restaurant_data

            return None

        except Exception as e:
            print(f"‚ùå Erreur: {str(e)}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def search_competitors_nearby(self, center_lat: float, center_lng: float,
                                  competitor_name: str, radius_km: float = 1.0):
        """
        Rechercher des concurrents autour d'un point
        """
        cache_key = f"competitor_{competitor_name}_{center_lat}_{center_lng}_{radius_km}".replace(' ', '_').replace('.', '_')

        cached = self._load_from_cache(cache_key)
        if cached:
            return cached

        print(f"üîç Recherche {competitor_name}")

        try:
            radius_meters = int(radius_km * 1000)

            results = self.gmaps.places_nearby(
                location=(center_lat, center_lng),
                radius=radius_meters,
                name=competitor_name,  # ‚úÖ name au lieu de keyword pour √™tre strict
                language='fr'
            )

            places = []

            for place in results.get('results', []):
                # ‚úÖ Filtrer : garder seulement si le nom contient vraiment le concurrent
                if competitor_name.lower() in place.get('name', '').lower():
                    places.append({
                        'name': place.get('name'),
                        'place_id': place.get('place_id'),
                        'formatted_address': place.get('formatted_address'),
                        'lat': place['geometry']['location']['lat'],
                        'lng': place['geometry']['location']['lng'],
                        'rating': place.get('rating'),
                        'user_ratings_total': place.get('user_ratings_total'),
                        'competitor': competitor_name
                    })

            self._save_to_cache(cache_key, places)
            print(f"‚úÖ Trouv√© {len(places)} {competitor_name}")

            return places

        except Exception as e:
            print(f"‚ùå Erreur: {str(e)}")
            return []

    def get_reviews(self, place_id: str):
        """R√©cup√©rer les avis d'un place_id"""
        data = self._get_place_data(place_id)
        return data.get('reviews', []) if data else []

print("‚úÖ Classe cr√©√©e avec filtre anti-gares")

"""## Chargement Fichier"""

# ============================================================================
# CHARGEMENT DES DONN√âES - TABLEAU INITIAL + BDD EXTERNE
# ============================================================================

# 1. Charger le tableau initial (vos restaurants)
df_initial = pd.read_excel('/content/drive/MyDrive/AREAS/AREAS_pdv_gares.xlsx')

# Renommer la colonne pour coh√©rence
df_initial = df_initial.rename(columns={'Point de vente / CDPF': 'PDV'})

# Normaliser les noms de gares dans le tableau initial
df_initial['Gare_Normalized'] = df_initial['Gare'].apply(normalize_gare_name)

print(f"üìä Tableau initial charg√©: {len(df_initial)} restaurants")
print(f"üìã Colonnes: {list(df_initial.columns)}")

# 2. Charger la BDD externe AREAS (CSV avec d√©tection du s√©parateur)
try:
    # Essayer avec point-virgule (format Excel fran√ßais)
    df_bdd = pd.read_csv('/content/drive/MyDrive/AREAS/AREAS_data_externe.csv',
                         sep=';',
                         encoding='utf-8',
                         on_bad_lines='skip')  # Ignorer les lignes probl√©matiques
    print("‚úÖ CSV charg√© avec s√©parateur ';'")
except:
    try:
        # Essayer avec virgule (format standard)
        df_bdd = pd.read_csv('/content/drive/MyDrive/AREAS/AREAS_data_externe.csv',
                             sep=',',
                             encoding='utf-8',
                             on_bad_lines='skip')
        print("‚úÖ CSV charg√© avec s√©parateur ','")
    except:
        # Essayer avec tabulation
        df_bdd = pd.read_csv('/content/drive/MyDrive/AREAS/AREAS_data_externe.csv',
                             sep='\t',
                             encoding='utf-8',
                             on_bad_lines='skip')
        print("‚úÖ CSV charg√© avec s√©parateur '\\t' (tabulation)")

print(f"\nüìä BDD externe charg√©e: {len(df_bdd)} lignes")
print(f"üìã Colonnes BDD: {list(df_bdd.columns)}")

# V√©rifier que les colonnes n√©cessaires existent
if 'Site' not in df_bdd.columns:
    print("‚ö†Ô∏è ATTENTION: Colonne 'Site' non trouv√©e")
    print("   Colonnes disponibles:", list(df_bdd.columns))

# Affichage
print("\nüìã Aper√ßu tableau initial:")
display(df_initial.head())

print("\nüìã Aper√ßu BDD externe:")
display(df_bdd.head())

"""## Recherche AREAS"""

# ============================================================================
# FONCTION DE MATCHING ET FUSION DES AVIS - GMAPS EN PRIORIT√â
# ============================================================================

def get_combined_reviews(pdv: str, gare: str, enseigne: str, gmaps_reviews: list, df_bdd: pd.DataFrame, max_avis: int = 20) -> tuple:
    """
    R√©cup√®re et combine les avis de Google Maps + BDD externe
    PRIORIT√â: Google Maps d'abord, puis BDD pour compl√©ter
    LIMITE: max_avis total (par d√©faut 20)

    Returns:
        tuple: (liste_avis_combin√©s, count_bdd, count_gmaps)
    """
    combined_reviews = []
    count_bdd = 0
    count_gmaps = 0

    # 1. D'ABORD : Ajouter les avis Google Maps (PRIORIT√â)
    if gmaps_reviews:
        # Limiter √† max_avis
        gmaps_to_add = gmaps_reviews[:max_avis]

        for review in gmaps_to_add:
            combined_reviews.append({
                'text': review.get('text', ''),
                'source': 'Google_Maps',
                'rating': review.get('rating'),
                'date': review.get('date')
            })
            count_gmaps += 1

    # 2. ENSUITE : Compl√©ter avec la BDD si on n'a pas encore 20 avis
    remaining_slots = max_avis - count_gmaps

    if remaining_slots > 0:
        # Normaliser le nom de gare pour le matching
        gare_normalized = normalize_gare_name(gare)

        # Chercher dans la BDD externe
        enseigne_patterns = [
            enseigne,
            enseigne.replace(' ', '-'),
            enseigne.replace(' ', ''),
        ]

        mask = pd.Series([False] * len(df_bdd))

        for pattern in enseigne_patterns:
            mask_temp = (df_bdd['Site'].str.contains(gare, case=False, na=False)) & \
                        (df_bdd['Site'].str.contains(pattern, case=False, na=False))
            mask = mask | mask_temp

        bdd_matches = df_bdd[mask]

        if len(bdd_matches) > 0:
            print(f"   ‚úÖ {len(bdd_matches)} match(s) trouv√©(s) dans BDD externe")

            all_bdd_reviews = []

            for _, row in bdd_matches.iterrows():
                if 'Avis' in row and pd.notna(row['Avis']):
                    avis_text = str(row['Avis'])
                    avis_lines = [line.strip() for line in avis_text.split('\n') if line.strip()]

                    for avis_line in avis_lines:
                        all_bdd_reviews.append({
                            'text': avis_line,
                            'source': 'BDD_AREAS',
                            'rating': None,
                            'date': None
                        })

            # Limiter aux slots restants
            import random
            if len(all_bdd_reviews) > remaining_slots:
                all_bdd_reviews = random.sample(all_bdd_reviews, remaining_slots)

            combined_reviews.extend(all_bdd_reviews)
            count_bdd = len(all_bdd_reviews)
        else:
            print(f"   ‚ö†Ô∏è Aucun match dans BDD externe")

    print(f"   üìä Total avis (LIMIT√â √† {max_avis}): {count_gmaps} (GMaps) + {count_bdd} (BDD) = {len(combined_reviews)}")

    return combined_reviews, count_bdd, count_gmaps

print("‚úÖ Fonction de fusion des avis OK (GMAPS EN PRIORIT√â, MAX 20)")

CACHE_PATH = '/content/drive/MyDrive/AREAS/cache_gmaps_extractions.xlsx'

def save_gmaps_results(df_results: pd.DataFrame, path: str = CACHE_PATH):
    """
    Sauvegarder les r√©sultats d'extraction Google Maps
    """
    try:
        df_results.to_excel(path, index=False)
        print(f"üíæ Cache Google Maps sauvegard√©: {path}")
        print(f"   üìä {len(df_results)} restaurants sauvegard√©s")
        return True
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde cache: {e}")
        return False

def load_gmaps_results(path: str = CACHE_PATH) -> pd.DataFrame:
    """
    Charger les r√©sultats d'extraction Google Maps depuis le cache
    """
    if os.path.exists(path):
        try:
            df = pd.read_excel(path)
            print(f"‚úÖ Cache Google Maps charg√©: {path}")
            print(f"   üìä {len(df)} restaurants charg√©s")
            print(f"   üìã Colonnes: {list(df.columns)[:5]}...")
            return df
        except Exception as e:
            print(f"‚ùå Erreur chargement cache: {e}")
            return None
    else:
        print(f"‚ö†Ô∏è Pas de cache trouv√© √†: {path}")
        return None

print("‚úÖ Syst√®me de cache Google Maps OK")

service = GoogleMapsReviewsService(GOOGLE_MAPS_API_KEY)

results = []
used_place_ids = set()

# ‚öôÔ∏è CONFIGURATION : Choisir le mode
USE_CACHE = True  # ‚Üê Mettre True pour utiliser le cache, False pour extraire

if USE_CACHE:
    print("üìÇ Mode: CHARGEMENT DEPUIS LE CACHE")
    df_results = load_gmaps_results()

    if df_results is not None:
        print("‚úÖ Extraction Google Maps charg√©e depuis le cache")
        print("   ‚Üí Pas besoin de refaire les appels API Google Maps!\n")

        # Afficher un r√©sum√©
        print("="*60)
        print(f"üìä R√©sum√© du cache:")
        print(f"   ‚Ä¢ Total restaurants: {len(df_results)}")
        print(f"   ‚Ä¢ Avec r√©sultats Google: {df_results['google_name'].notna().sum()}")
        print(f"   ‚Ä¢ Non trouv√©s: {df_results['google_name'].isna().sum()}")
        print("="*60 + "\n")
    else:
        print("‚ö†Ô∏è Impossible de charger le cache, extraction n√©cessaire")
        USE_CACHE = False

if not USE_CACHE:
    print("üîç Mode: NOUVELLE EXTRACTION GOOGLE MAPS\n")

    service = GoogleMapsReviewsService(GOOGLE_MAPS_API_KEY)

    results = []
    used_place_ids = set()

    for idx, row in df_initial.iterrows():
        pdv = row.get('PDV')
        gare = row.get('Gare')  # "PARIS-GDE"
        gare_normalized = row.get('Gare_Normalized')  # "Gare de l'Est"
        enseigne = row.get('Enseigne')

        print(f"\n[{idx + 1}/{len(df_initial)}] {enseigne} - {gare}")

        # ‚úÖ Utiliser gare_normalized pour la recherche Google Maps
        restaurant = service.find_restaurant(
            row.get('PDV', ''),
            row.get('Gare_Normalized', ''),
            exclude_place_ids=used_place_ids
        )

        if restaurant:
            # ‚úÖ Ajouter ce place_id aux utilis√©s
            used_place_ids.add(restaurant.get('place_id'))

            reviews = restaurant.get('reviews', [])

            # R√©cup√©rer les avis Google Maps
            gmaps_reviews_list = reviews if isinstance(reviews, list) else []

            # Combiner avec la BDD externe
            combined_reviews, count_bdd, count_gmaps = get_combined_reviews(
                pdv=row.get('PDV'),
                gare=row.get('Gare'),
                enseigne=row.get('Enseigne'),
                gmaps_reviews=gmaps_reviews_list,
                df_bdd=df_bdd
            )

            # Sauvegarder les avis combin√©s
            reviews_combined_json = json.dumps(combined_reviews, ensure_ascii=False)
            reviews_count_total = len(combined_reviews)

            print(f"   üíæ Total: {count_bdd} (BDD) + {count_gmaps} (GMaps) = {reviews_count_total} avis")

            result = {
                **row.to_dict(),
                'google_place_id': restaurant.get('place_id'),
                'google_name': restaurant.get('name'),
                'google_address': restaurant.get('formatted_address'),
                'google_lat': restaurant.get('lat'),
                'google_lng': restaurant.get('lng'),
                'google_rating': restaurant.get('rating'),
                'google_total_ratings': restaurant.get('user_ratings_total'),
                'google_phone': restaurant.get('phone'),
                'google_website': restaurant.get('website'),
                'google_url': restaurant.get('url'),
                'is_duplicate': restaurant.get('is_duplicate', False),

                'gmaps_reviews_count': count_gmaps,
                'bdd_reviews_count': count_bdd,
                'reviews_count_total': reviews_count_total,
                'reviews_combined_json': reviews_combined_json,

                'reviews_json': json.dumps(reviews, ensure_ascii=False)
            }

            for i, review in enumerate(reviews, 1):
                result[f'review_{i}_author'] = review.get('author_name')
                result[f'review_{i}_rating'] = review.get('rating')
                result[f'review_{i}_text'] = review.get('text')
                result[f'review_{i}_date'] = datetime.fromtimestamp(review.get('time', 0)).isoformat()
        else:
            result = {**row.to_dict(), 'error': 'Non trouv√©'}

        results.append(result)
        time.sleep(0.5)

    df_results = pd.DataFrame(results)

    # ‚úÖ SAUVEGARDER LE CACHE AVEC TOUT (GMaps + BDD d√©j√† fusionn√©s)
    save_gmaps_results(df_results)

    print(f"\nüíæ Cache sauvegard√©: {len(df_results)} restaurants")
    print(f"   ‚úÖ Inclus: extractions GMaps + avis BDD fusionn√©s")

    print(f"\n{'='*60}")
    print(f"‚úÖ {len(df_results)} restaurants trait√©s")
    print(f"‚ö†Ô∏è {df_results['is_duplicate'].sum() if 'is_duplicate' in df_results else 0} doublons d√©tect√©s")
    print(f"{'='*60}\n")

# ========== R√âSUM√â FINAL (avec ou sans cache) ==========

print(f"\n{'='*60}")
print(f"üìä R√âSUM√â FINAL:")
print(f"   ‚Ä¢ Restaurants: {len(df_results)}")

# V√©rifier si les colonnes existent avant de les afficher
if 'gmaps_reviews_count' in df_results.columns:
    print(f"   ‚Ä¢ Avis GMaps: {df_results['gmaps_reviews_count'].sum():.0f}")
if 'bdd_reviews_count' in df_results.columns:
    print(f"   ‚Ä¢ Avis BDD: {df_results['bdd_reviews_count'].sum():.0f}")
if 'reviews_count_total' in df_results.columns:
    print(f"   ‚Ä¢ Total combin√©: {df_results['reviews_count_total'].sum():.0f}")

print(f"{'='*60}\n")

# Afficher r√©sum√© d√©taill√©
print("üìã Aper√ßu des restaurants:\n")
for idx, row in df_results.head(10).iterrows():
    print(f"[{idx+1}] {row.get('Enseigne')} - {row.get('Gare')}")
    if row.get('google_name'):
        duplicate = "‚ö†Ô∏è DOUBLON" if row.get('is_duplicate') else ""
        print(f"    ‚úÖ {row.get('google_name')} {duplicate}")
        print(f"    üìç {row.get('google_address')}")
        print(f"    üí¨ GMaps: {row.get('gmaps_reviews_count', 0)} | BDD: {row.get('bdd_reviews_count', 0)} | Total: {row.get('reviews_count_total', 0)} avis\n")
    else:
        print(f"    ‚ùå Non trouv√©\n")

# Afficher DataFrame final
print("\nüìä DataFrame final:")
display(df_results[['Enseigne', 'Gare', 'google_name', 'gmaps_reviews_count',
                     'bdd_reviews_count', 'reviews_count_total']].head(10))

# Check the initial number of rows
initial_rows = len(df_results)
print(f"Initial number of rows in df_results: {initial_rows}")

# Remove rows where 'reviews_json' is empty
df_results_cleaned = df_results[df_results['reviews_json'].str.len() > 2].copy() # Assuming '[]' is the representation of empty

# Calculate the number of dropped rows
dropped_rows = initial_rows - len(df_results_cleaned)

print(f"Number of rows dropped: {dropped_rows}")
print(f"Number of rows after dropping empty reviews: {len(df_results_cleaned)}")

# Update df_results to the cleaned version if you want to use it later
df_results = df_results_cleaned

display(df_results.head())

"""## Recherche Concurrentielle"""

CACHE_COMPETITIVE_PATH = '/content/drive/MyDrive/AREAS/cache_competitive_search.xlsx'

def save_competitive_results(df_competitive, path=CACHE_COMPETITIVE_PATH):
    df_competitive.to_excel(path, index=False)
    print(f"üíæ Cache concurrentiel sauvegard√©: {len(df_competitive)} restaurants")

def load_competitive_results(path=CACHE_COMPETITIVE_PATH):
    if os.path.exists(path):
        df = pd.read_excel(path)
        print(f"‚úÖ Cache charg√©: {len(df)} restaurants")
        return df
    return None

USE_COMPETITIVE_CACHE = True  # ‚Üê True = cache, False = recherche

if USE_COMPETITIVE_CACHE:
    df_competitive = load_competitive_results()
    if df_competitive is None:
        USE_COMPETITIVE_CACHE = False

if not USE_COMPETITIVE_CACHE:
    # Recherche des concurrents
    COMPETITORS = ["Brioche Dor√©e", "Pr√™t √† Manger", "Relay"]
    RADIUS_KM = 1.0  # Rayon de recherche

    all_competitors = []

    for idx, row in df_results.iterrows():
        if pd.isna(row.get('google_lat')) or pd.isna(row.get('google_lng')):
            continue

        center_lat = row['google_lat']
        center_lng = row['google_lng']
        location_name = f"{row.get('Enseigne')} - {row.get('Gare')}"

        print(f"\n{'='*60}")
        print(f"[{idx + 1}/{len(df_results)}] Concurrents autour de: {location_name}")
        print(f"{'='*60}")

        for competitor in COMPETITORS:
            competitors_found = service.search_competitors_nearby(
                center_lat,
                center_lng,
                competitor,
                RADIUS_KM
            )

            for comp in competitors_found[:3]:
                # R√©cup√©rer les avis du concurrent (max 5)
                reviews = service.get_reviews(comp['place_id'])

                competitor_data = {
                    'reference_location': location_name,
                    'reference_lat': center_lat,
                    'reference_lng': center_lng,
                    **comp,
                    'reviews_count': len(reviews),
                    'reviews_json': json.dumps(reviews, ensure_ascii=False)
                }

                # Ajouter les avis
                for i, review in enumerate(reviews, 1):
                    competitor_data[f'review_{i}_author'] = review.get('author_name')
                    competitor_data[f'review_{i}_rating'] = review.get('rating')
                    competitor_data[f'review_{i}_text'] = review.get('text')
                    competitor_data[f'review_{i}_date'] = review.get('time')

                all_competitors.append(competitor_data)
                print(f"  ‚úÖ {comp['name']}: {len(reviews)} avis")

            time.sleep(0.5)

    # Cr√©er DataFrame des concurrents
    df_competitors = pd.DataFrame(all_competitors)

    print(f"\n{'='*60}")
    print(f"‚úÖ TERMIN√â: {len(df_competitors)} lieux concurrents trouv√©s")
    print(f"{'='*60}")

    # ‚úÖ CORRECTION ICI
    df_competitive = df_competitors  # Utiliser df_competitors, pas competitive_results

    # Sauvegarder dans le cache
    save_competitive_results(df_competitive)

    print(f"\nüíæ Cache sauvegard√©")

# Afficher le r√©sultat (avec ou sans cache)
print(f"\nüìä Aper√ßu des concurrents:")
display(df_competitive.head())

"""## Sauvegarde"""

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# Sauvegarder vos restaurants enrichis
output_path_1 = f'/content/drive/MyDrive/AREAS/restaurants_enriched_{timestamp}.xlsx'
df_results.to_excel(output_path_1, index=False)
print(f"‚úÖ Vos restaurants sauvegard√©s: {output_path_1}")

# Sauvegarder les concurrents
output_path_2 = f'/content/drive/MyDrive/AREAS/competitors_{timestamp}.xlsx'
df_competitors.to_excel(output_path_2, index=False)
print(f"‚úÖ Concurrents sauvegard√©s: {output_path_2}")

print(f"\nüéâ EXTRACTION TERMIN√âE !")
print(f"‚ö†Ô∏è RAPPEL: Maximum 5 avis par lieu (limitation API Google)")

"""# Analyses des Reviews

## Fonctions
"""

ANTHROPIC_API_KEY = userdata.get("ANTH")  # from your Secrets panel

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

# Configurer Anthropic
client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

import re

def analyze_restaurant_reviews(restaurant_name: str, reviews_list: list) -> dict:
    """
    Analyser les avis d'un restaurant selon les dimensions d√©finies
    VERSION CORRIG√âE POUR HAIKU - G√®re le texte suppl√©mentaire apr√®s le JSON
    """

    # Extraire les textes des avis
    reviews_texts = []
    for r in reviews_list:
        if isinstance(r, dict) and r.get('text'):
            source = r.get('source', 'Unknown')
            reviews_texts.append(f"[{source}] {r.get('text')}")
        elif isinstance(r, str):
            reviews_texts.append(r)

    if not reviews_texts:
        return None

    # Pr√©parer le prompt
    reviews_str = "\n\n".join([f"Avis {i+1}: {text}" for i, text in enumerate(reviews_texts)])

    prompt = f"""Tu es un analyste expert en restauration. Analyse les avis suivants pour le restaurant "{restaurant_name}".

AVIS √Ä ANALYSER:
{reviews_str}

DIMENSIONS √Ä NOTER (1-5 ou "N/A" si pas d'info):

1. Qualit√© Nourriture (qualit√©_nourriture): Fra√Æcheur, go√ªt, qualit√© des ingr√©dients
2. Vari√©t√© Menu (variete_menu): Diversit√© des options, choix disponibles
3. Rapport Qualit√©-Prix (rapport_qualite_prix): Prix par rapport √† la qualit√©/quantit√©
4. Service Rapidit√© (service_rapidite): Vitesse de service
5. Service Amabilit√© (service_amabilite): Courtoisie et accueil du personnel
6. Propret√© (proprete): Hygi√®ne du lieu
7. Ambiance (ambiance): Atmosph√®re g√©n√©rale
8. Accessibilit√© (accessibilite): Facilit√© d'acc√®s, accessibilit√© PMR
9. Pratique Gare (pratique_gare): Pratique pour les voyageurs
10. Attente (attente): Temps d'attente
11. Fra√Æcheur Produits (fraicheur_produits): Fra√Æcheur sp√©cifique
12. Options Saines (options_saines): Disponibilit√© options healthy
13. Diversit√© R√©gimes (diversite_regimes): V√©g√©tarien, sans gluten, etc.
14. Confort (confort): Confort des si√®ges, espace

IMPORTANT:
- R√©ponds UNIQUEMENT avec un objet JSON valide
- Pas de texte avant ou apr√®s le JSON
- Format exact attendu:

{{
  "qualite_nourriture": 4,
  "variete_menu": 3,
  "rapport_qualite_prix": "N/A",
  ...
}}"""

    try:
        message = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )

        # ‚úÖ NOUVEAU: Extraction robuste du JSON
        response_text = message.content[0].text.strip()

        # Chercher les accolades du JSON
        try:
            start = response_text.find('{')
            end = response_text.rfind('}')

            if start != -1 and end != -1:
                json_str = response_text[start:end+1]
                result = json.loads(json_str)
            else:
                raise ValueError("Pas de JSON dans la r√©ponse")

        except (json.JSONDecodeError, ValueError) as parse_error:
            # Fallback: nettoyer les blocs markdown
            cleaned = re.sub(r'```json\s*', '', response_text)
            cleaned = re.sub(r'```\s*', '', cleaned).strip()

            try:
                result = json.loads(cleaned)
            except json.JSONDecodeError:
                print(f"‚ùå Erreur parsing JSON: {str(parse_error)}")
                print(f"üìÑ R√©ponse re√ßue: {response_text[:300]}...")
                return None

        return result

    except Exception as e:
        print(f"‚ùå Erreur analyse AI: {str(e)}")
        return None

# ============================================================================
# SYST√àME DE CACHE POUR L'ANALYSE IA
# ============================================================================

CACHE_AI_DIR = '/content/drive/MyDrive/AREAS/cache_ai/'

# Cr√©er le r√©pertoire si n√©cessaire
Path(CACHE_AI_DIR).mkdir(parents=True, exist_ok=True)

def get_latest_ai_cache():
    """
    Trouver le fichier de cache IA le plus r√©cent
    """
    cache_files = glob.glob(f"{CACHE_AI_DIR}cache_ai_analysis_*.xlsx")

    if not cache_files:
        return None

    # Trier par date de modification (le plus r√©cent en premier)
    latest_file = max(cache_files, key=os.path.getmtime)
    return latest_file

def save_ai_analysis(df_analyzed: pd.DataFrame):
    """
    Sauvegarder les analyses IA avec timestamp
    """
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filepath = f"{CACHE_AI_DIR}cache_ai_analysis_{timestamp}.xlsx"

        df_analyzed.to_excel(filepath, index=False)

        print(f"üíæ Cache analyse IA sauvegard√©: {filepath}")
        print(f"   üìä {len(df_analyzed)} restaurants analys√©s")
        print(f"   üïê Timestamp: {timestamp}")
        return True
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde cache IA: {e}")
        return False

def load_ai_analysis():
    """
    Charger la plus r√©cente analyse IA
    """
    latest_file = get_latest_ai_cache()

    if latest_file is None:
        print(f"‚ö†Ô∏è Aucun cache d'analyse IA trouv√© dans {CACHE_AI_DIR}")
        return None

    try:
        df = pd.read_excel(latest_file)

        # Extraire le timestamp du nom du fichier
        filename = os.path.basename(latest_file)
        timestamp = filename.replace('cache_ai_analysis_', '').replace('.xlsx', '')

        print(f"‚úÖ Cache analyse IA charg√©: {filename}")
        print(f"   üìä {len(df)} restaurants analys√©s")
        print(f"   üïê Date: {timestamp}")
        print(f"   üìã Colonnes: {len(df.columns)}")

        return df
    except Exception as e:
        print(f"‚ùå Erreur chargement cache IA: {e}")
        return None

def list_ai_caches():
    """
    Lister tous les caches IA disponibles
    """
    cache_files = glob.glob(f"{CACHE_AI_DIR}cache_ai_analysis_*.xlsx")

    if not cache_files:
        print("‚ö†Ô∏è Aucun cache d'analyse IA trouv√©")
        return []

    print(f"üìÅ {len(cache_files)} cache(s) d'analyse IA trouv√©(s):\n")

    # Trier par date (plus r√©cent en premier)
    cache_files.sort(key=os.path.getmtime, reverse=True)

    for i, filepath in enumerate(cache_files, 1):
        filename = os.path.basename(filepath)
        timestamp = filename.replace('cache_ai_analysis_', '').replace('.xlsx', '')
        mod_time = datetime.fromtimestamp(os.path.getmtime(filepath))
        size_kb = os.path.getsize(filepath) / 1024

        marker = "üåü PLUS R√âCENT" if i == 1 else ""
        print(f"  [{i}] {filename} {marker}")
        print(f"      üìÖ {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"      üíæ {size_kb:.2f} KB\n")

    return cache_files

print("‚úÖ Syst√®me de cache analyse IA OK")

"""## Analyse AREAS avec IA"""

# ============================================================================
# ANALYSE IA AVEC CACHE
# ============================================================================

# ‚öôÔ∏è CONFIGURATION
USE_AI_CACHE = False  # ‚Üê True = utiliser cache, False = nouvelle analyse

if USE_AI_CACHE:
    print("üìÇ Mode: CHARGEMENT CACHE ANALYSE IA\n")
    df_analyzed = load_ai_analysis()

    if df_analyzed is not None:
        print("\n‚úÖ Analyses IA charg√©es depuis le cache")
        print("   ‚Üí Pas besoin de refaire les analyses Claude!\n")

        # Afficher un r√©sum√©
        print("="*60)
        print(f"üìä R√©sum√© du cache IA:")
        print(f"   ‚Ä¢ Restaurants analys√©s: {len(df_analyzed)}")

        # Compter les restaurants avec analyses compl√®tes
        note_columns = [
            'OFFRE - Profondeur de gamme', 'NOURRITURE - Qualit√©',
            'PRIX - Niveau global', 'RAPIDIT√â - Service'
        ]
        complete = df_analyzed[note_columns].notna().all(axis=1).sum()
        print(f"   ‚Ä¢ Analyses compl√®tes: {complete}")

        print("="*60 + "\n")
    else:
        print("‚ö†Ô∏è Impossible de charger le cache, analyse n√©cessaire")
        USE_AI_CACHE = False

if not USE_AI_CACHE:
    print("ü§ñ Mode: NOUVELLE ANALYSE IA\n")
    print("ü§ñ Analyse AI des restaurants en cours...\n")

    analyzed_restaurants = []

    # Utiliser df_results (qui contient d√©j√† reviews_combined_json)
    for idx, row in df_results.iterrows():
        restaurant_name = row.get('google_name') or row.get('Enseigne')
        reviews_json = row.get('reviews_combined_json')  # ‚Üê Utiliser avis combin√©s

        print(f"[{idx+1}/{len(df_results)}] Analyse: {restaurant_name}")

        if not reviews_json or reviews_json == '[]':
            print(f"   ‚ö†Ô∏è Pas d'avis, skip")
            continue

        # Parser les avis
        try:
            reviews_list = json.loads(reviews_json)
        except:
            print(f"   ‚ùå Erreur parsing JSON")
            continue

        # Analyser avec AI
        analysis = analyze_restaurant_reviews(restaurant_name, reviews_list)

        if analysis:
            # Combiner donn√©es originales + analyse
            analyzed_row = {
                'PDV': row.get('PDV'),
                'Gare': row.get('Gare'),
                'Enseigne': row.get('Enseigne'),
                'google_name': row.get('google_name'),
                'google_address': row.get('google_address'),
                'google_rating': row.get('google_rating'),
                'google_total_ratings': row.get('google_total_ratings'),
                'gmaps_reviews_count': row.get('gmaps_reviews_count'),
                'bdd_reviews_count': row.get('bdd_reviews_count'),
                'reviews_count_total': row.get('reviews_count_total'),

                # Notes AI (1-5 ou N/A)
                'OFFRE - Profondeur de gamme': analysis.get('offre_profondeur'),
                'OFFRE - Renouvellement': analysis.get('offre_renouvellement'),
                'OFFRE - Clart√©': analysis.get('offre_clarte'),

                'NOURRITURE - Qualit√©': analysis.get('nourriture_qualite'),
                'NOURRITURE - Quantit√©': analysis.get('nourriture_quantite'),
                'NOURRITURE - Pr√©sentation': analysis.get('nourriture_presentation'),

                'PRIX - Niveau global': analysis.get('prix_niveau_global'),
                'PRIX - Niveau menus': analysis.get('prix_niveau_menus'),
                'PRIX - Rapport qualit√©/prix': analysis.get('prix_rapport_qualite'),

                'RAPIDIT√â - Service': analysis.get('rapidite_service'),

                'ATMOSPH√àRE - Entretien': analysis.get('atmosphere_entretien'),
                'ATMOSPH√àRE - Confort': analysis.get('atmosphere_confort'),

                'FORCE DE VENTE - Personnel': analysis.get('force_vente'),

                'HYGI√àNE': analysis.get('hygiene')
            }

            analyzed_restaurants.append(analyzed_row)
            print(f"   ‚úÖ Analys√©")
        else:
            print(f"   ‚ùå √âchec analyse")

        # Pause pour √©viter rate limits
        time.sleep(1)

    # Cr√©er DataFrame
    df_analyzed = pd.DataFrame(analyzed_restaurants)

    # Conversion num√©rique des notes
    note_columns = [
        'OFFRE - Profondeur de gamme', 'OFFRE - Renouvellement', 'OFFRE - Clart√©',
        'NOURRITURE - Qualit√©', 'NOURRITURE - Quantit√©', 'NOURRITURE - Pr√©sentation',
        'PRIX - Niveau global', 'PRIX - Niveau menus', 'PRIX - Rapport qualit√©/prix',
        'RAPIDIT√â - Service', 'ATMOSPH√àRE - Entretien', 'ATMOSPH√àRE - Confort',
        'FORCE DE VENTE - Personnel', 'HYGI√àNE'
    ]

    for col in note_columns:
        if col in df_analyzed.columns:
            df_analyzed[col] = pd.to_numeric(df_analyzed[col], errors='coerce')

    # Sauvegarder dans le cache
    save_ai_analysis(df_analyzed)

    print(f"\n{'='*60}")
    print(f"‚úÖ {len(df_analyzed)} restaurants analys√©s")
    print(f"{'='*60}\n")

# ========== AFFICHAGE R√âSULTATS (avec ou sans cache) ==========

print("\nüìä Aper√ßu des r√©sultats d'analyse:")
display(df_analyzed.head(10))

print(f"\nüìà Statistiques des notes:")
note_cols = [col for col in df_analyzed.columns if col in note_columns]
if note_cols:
    stats = df_analyzed[note_cols].describe()
    display(stats.loc[['mean', 'std', 'min', 'max']])

"""## Sauvegarde des R√©sultats AREAS"""

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_path = f'/content/drive/MyDrive/AREAS/restaurants_analyzed_{timestamp}.xlsx'

df_analyzed.to_excel(output_path, index=False)

print(f"\nüíæ Fichier sauvegard√©: {output_path}")
print(f"\n‚úÖ TERMIN√â!")
print(f"\nüìã Structure du fichier:")
print(f"   - {len(df_analyzed)} lignes (restaurants)")
print(f"   - {len(df_analyzed.columns)} colonnes")
print(f"   - Colonnes de notes: 14 dimensions (1-5 ou N/A)")

